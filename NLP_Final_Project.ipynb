{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# /**\n",
    "#  * Project Name: Automated News Headline Generation Using Natural Language\n",
    "#  * File Name: NLP_Final_Project.ipynb\n",
    "#  * \n",
    "#  * Description:\n",
    "#  * This project preprocesses news articles using spaCy and generates concise, informative headlines for each article.\n",
    "#  * It evaluates the quality of the generated headlines using cosine similarity with the original title.\n",
    "#  * \n",
    "#  * Contributors:\n",
    "#  * - Grace Li - Handled data preprocessing and integration of TF-IDF for evaluation.\n",
    "#  * - Nick Yi - Focused on implementing tokenization and cosine similarity evaluation.\n",
    "#  * - Golam Raiyan - Developed headline generation logic and named entity recognition (NER) functionality.\n",
    "#  * \n",
    "#  * Date: 12/12/2024\n",
    "#  */\n",
    "\n",
    "#install necessary libraries\n",
    "!pip install spacy\n",
    "!pip install pandas scikit-learn\n",
    "!python -m spacy download en_core_web_md\n",
    "\n",
    "#import required modules\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import spacy\n",
    "\n",
    "#load the spaCy model for natural language processing\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "#reads and processes articles from a CSV file\n",
    "#extracts token data and named entities using spaCy\n",
    "def process_articles(input_file):\n",
    "    df = pd.read_csv(input_file)\n",
    "    processed_articles = []\n",
    "    for _, row in df.iterrows():\n",
    "        article_number = row.get(\"article_number\", _)\n",
    "        text = row[\"text\"]\n",
    "        doc = nlp(text)\n",
    "        token_data = [(token.text, token.pos_) for token in doc]\n",
    "        named_entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "        processed_articles.append({\n",
    "            \"article_number\": article_number,\n",
    "            \"original_text\": text,\n",
    "            \"token_data\": token_data,\n",
    "            \"named_entities\": named_entities\n",
    "        })\n",
    "    return processed_articles\n",
    "\n",
    "#generates headlines for articles based on content\n",
    "def generate_headlines(processed_articles):\n",
    "    headlines = []\n",
    "    for article in processed_articles:\n",
    "        doc = nlp(article[\"original_text\"])\n",
    "        important_phrases = []\n",
    "        for token in doc:\n",
    "            if token.dep_ in {\"nsubj\", \"ROOT\", \"dobj\", \"pobj\"} or token.ent_type_:\n",
    "                important_phrases.append(token.text)\n",
    "        named_entities = [ent.text for ent in doc.ents if ent.label_ in {\"PERSON\", \"ORG\", \"GPE\"}]\n",
    "        headline = \" \".join(important_phrases[:15])\n",
    "        if named_entities:\n",
    "            headline += \" - \" + \", \".join(named_entities[:2])\n",
    "        headlines.append({\n",
    "            \"article_number\": article[\"article_number\"],\n",
    "            \"headline\": headline.capitalize()\n",
    "        })\n",
    "    return headlines\n",
    "\n",
    "#calculates cosine similarity using TF-IDF\n",
    "def calculate_cosine_similarity(original_texts, generated_headlines):\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    original_texts = [text for text in original_texts]\n",
    "    generated_texts = [headline[\"headline\"] for headline in generated_headlines]\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(original_texts + generated_texts)\n",
    "    original_matrix = tfidf_matrix[:len(original_texts)]\n",
    "    headline_matrix = tfidf_matrix[len(original_texts):]\n",
    "    similarities = cosine_similarity(original_matrix, headline_matrix)\n",
    "    return similarities\n",
    "\n",
    "#computes the average cosine similarity\n",
    "def get_average_cosine_similarity(similarities):\n",
    "    total_similarity = sum(similarities[i][i] for i in range(len(similarities)))\n",
    "    average_similarity = total_similarity / len(similarities)\n",
    "    return average_similarity\n",
    "\n",
    "#driver function\n",
    "def main(input_file, output_file):\n",
    "    print(\"Processing articles...\")\n",
    "    processed_articles = process_articles(input_file)\n",
    "    print(\"Generating headlines...\")\n",
    "    generated_headlines = generate_headlines(processed_articles)\n",
    "    print(\"Calculating cosine similarity...\")\n",
    "    similarities = calculate_cosine_similarity(\n",
    "        [article[\"original_text\"] for article in processed_articles],\n",
    "        generated_headlines\n",
    "    )\n",
    "    average_similarity = get_average_cosine_similarity(similarities)\n",
    "    print(f\"Average Cosine Similarity: {average_similarity:.4f}\")\n",
    "    results = []\n",
    "    for idx, headline in enumerate(generated_headlines):\n",
    "        results.append({\n",
    "            \"article_number\": headline[\"article_number\"],\n",
    "            \"generated_headline\": headline[\"headline\"],\n",
    "            \"cosine_similarity\": similarities[idx][idx]\n",
    "        })\n",
    "    \n",
    "    #save results to a CSV file\n",
    "    pd.DataFrame(results).to_csv(output_file, index=False)\n",
    "    print(f\"Results saved to {output_file}\")\n",
    "\n",
    "#input and output files\n",
    "if __name__ == \"__main__\":\n",
    "    main(\"test.csv\", \"headline_pipeline_results.csv\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
