{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary packages and SpaCy model in Colab\n",
    "!pip install spacy\n",
    "!pip install pandas scikit-learn\n",
    "!python -m spacy download en_core_web_md\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import spacy\n",
    "\n",
    "# Load SpaCy model\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "def process_articles(input_file):\n",
    "    \"\"\"\n",
    "    Step 1: Process articles for POS tagging and Named Entity Recognition (NER) using SpaCy.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(input_file)\n",
    "    processed_articles = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        article_number = row.get(\"article_number\", _)\n",
    "        text = row[\"text\"]\n",
    "        doc = nlp(text)\n",
    "\n",
    "        # Extract token and POS tag information\n",
    "        token_data = [(token.text, token.pos_) for token in doc]\n",
    "\n",
    "        # Extract named entities\n",
    "        named_entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "\n",
    "        processed_articles.append({\n",
    "            \"article_number\": article_number,\n",
    "            \"original_text\": text,\n",
    "            \"token_data\": token_data,\n",
    "            \"named_entities\": named_entities\n",
    "        })\n",
    "\n",
    "    return processed_articles\n",
    "\n",
    "def generate_headlines(processed_articles):\n",
    "    \"\"\"\n",
    "    Step 2: Generate grammatically coherent headlines using advanced logic and SpaCy POS tagging/NER.\n",
    "    \"\"\"\n",
    "    headlines = []\n",
    "    for article in processed_articles:\n",
    "        doc = nlp(article[\"original_text\"])\n",
    "\n",
    "        # Extract meaningful phrases (subjects, verbs, objects, modifiers)\n",
    "        important_phrases = []\n",
    "        for token in doc:\n",
    "            if token.dep_ in {\"nsubj\", \"ROOT\", \"dobj\", \"pobj\"} or token.ent_type_:\n",
    "                important_phrases.append(token.text)\n",
    "\n",
    "        # Include named entities for additional context\n",
    "        named_entities = [ent.text for ent in doc.ents if ent.label_ in {\"PERSON\", \"ORG\", \"GPE\"}]\n",
    "\n",
    "        # Construct headline with more complexity and 15-20 words\n",
    "        headline = \" \".join(important_phrases[:15])  # Limit to top 15 phrases\n",
    "        if named_entities:\n",
    "            headline += \" - \" + \", \".join(named_entities[:2])  # Add some context\n",
    "\n",
    "        headlines.append({\n",
    "            \"article_number\": article[\"article_number\"],\n",
    "            \"headline\": headline.capitalize()\n",
    "        })\n",
    "    return headlines\n",
    "\n",
    "def calculate_cosine_similarity(original_texts, generated_headlines):\n",
    "    \"\"\"\n",
    "    Step 3: Calculate cosine similarity between original text and generated headlines.\n",
    "    \"\"\"\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    original_texts = [text for text in original_texts]\n",
    "    generated_texts = [headline[\"headline\"] for headline in generated_headlines]\n",
    "\n",
    "    # Compute TF-IDF matrix\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(original_texts + generated_texts)\n",
    "    original_matrix = tfidf_matrix[:len(original_texts)]\n",
    "    headline_matrix = tfidf_matrix[len(original_texts):]\n",
    "\n",
    "    # Calculate cosine similarity\n",
    "    similarities = cosine_similarity(original_matrix, headline_matrix)\n",
    "    return similarities\n",
    "\n",
    "def get_average_cosine_similarity(similarities):\n",
    "    \"\"\"\n",
    "    Calculate the average cosine similarity across all articles.\n",
    "    \"\"\"\n",
    "    total_similarity = sum(similarities[i][i] for i in range(len(similarities)))\n",
    "    average_similarity = total_similarity / len(similarities)\n",
    "    return average_similarity\n",
    "\n",
    "def main(input_file, output_file):\n",
    "    \"\"\"\n",
    "    Run the full pipeline.\n",
    "    \"\"\"\n",
    "    print(\"Processing articles...\")\n",
    "    processed_articles = process_articles(input_file)\n",
    "\n",
    "    print(\"Generating headlines...\")\n",
    "    generated_headlines = generate_headlines(processed_articles)\n",
    "\n",
    "    print(\"Calculating cosine similarity...\")\n",
    "    similarities = calculate_cosine_similarity(\n",
    "        [article[\"original_text\"] for article in processed_articles],\n",
    "        generated_headlines\n",
    "    )\n",
    "\n",
    "    # Calculate average cosine similarity\n",
    "    average_similarity = get_average_cosine_similarity(similarities)\n",
    "    print(f\"Average Cosine Similarity: {average_similarity:.4f}\")\n",
    "\n",
    "    # Save results\n",
    "    results = []\n",
    "    for idx, headline in enumerate(generated_headlines):\n",
    "        results.append({\n",
    "            \"article_number\": headline[\"article_number\"],\n",
    "            \"generated_headline\": headline[\"headline\"],\n",
    "            \"cosine_similarity\": similarities[idx][idx]\n",
    "        })\n",
    "    pd.DataFrame(results).to_csv(output_file, index=False)\n",
    "    print(f\"Results saved to {output_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(\"subset_news_data_fifty.csv\", \"headline_pipeline_results.csv\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
